# Job Scraper Dashboard

Une application web moderne pour scraper et analyser les offres d'emploi depuis plusieurs plateformes (Indeed, LinkedIn, et sites propriÃ©taires).

## ğŸš€ FonctionnalitÃ©s

- **Multi-plateformes** : Scraping depuis Indeed, LinkedIn et sites propriÃ©taires
- **Interface moderne** : Dashboard React avec Tailwind CSS
- **Scraping intelligent** : DÃ©tection automatique des sites et adaptation du scraping
- **RÃ©sultats en temps rÃ©el** : Affichage immÃ©diat des rÃ©sultats
- **Export des donnÃ©es** : Sauvegarde automatique en JSON/JSONL
- **DÃ©ploiement cloud** : Compatible Railway, Vercel, et autres plateformes

## ğŸ› ï¸ Technologies

- **Frontend** : Next.js 15, React 19, TypeScript
- **Styling** : Tailwind CSS, shadcn/ui
- **Scraping** : Puppeteer, Playwright
- **Backend** : API Routes Next.js
- **DÃ©ploiement** : Docker, Railway

## ğŸ“¦ Installation

### PrÃ©requis
- Node.js 18+
- npm ou yarn

### Installation locale

```bash
# Cloner le repository
git clone <votre-repo>
cd job-scraper-app

# Installer les dÃ©pendances
npm install

# DÃ©marrer en mode dÃ©veloppement
npm run dev
```

L'application sera disponible sur `http://localhost:3000`

## ğŸš€ DÃ©ploiement

### Railway

1. Connectez votre repository GitHub Ã  Railway
2. Railway dÃ©tectera automatiquement le Dockerfile
3. L'application sera dÃ©ployÃ©e automatiquement

### Variables d'environnement

CrÃ©ez un fichier `.env.local` :

```env
# Configuration Puppeteer
PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser

# Configuration de l'application
NODE_ENV=production
```

## ğŸ“Š Utilisation

1. **SÃ©lection des plateformes** : Choisissez Indeed, LinkedIn, ou les deux
2. **Upload du CSV** : TÃ©lÃ©chargez votre fichier CSV avec les entreprises
3. **Lancement du scraping** : Cliquez sur "Lancer le scraping"
4. **Consultation des rÃ©sultats** : Les rÃ©sultats s'affichent en temps rÃ©el

## ğŸ”§ Configuration

### Format du CSV

Le fichier CSV doit contenir une colonne avec les noms d'entreprises :

```csv
entreprise
ACCESSITE
ORANGE
SFR
```

### Personnalisation des scrapers

Les scrapers sont modulaires et peuvent Ãªtre facilement Ã©tendus dans le dossier `scrapers/`.

## ğŸ“ Structure du projet

```
job-scraper-app/
â”œâ”€â”€ app/                    # Pages et API Next.js
â”‚   â”œâ”€â”€ api/               # Routes API
â”‚   â”œâ”€â”€ globals.css        # Styles globaux
â”‚   â”œâ”€â”€ layout.tsx         # Layout principal
â”‚   â””â”€â”€ page.tsx           # Page d'accueil
â”œâ”€â”€ components/            # Composants React
â”‚   â””â”€â”€ ui/               # Composants UI (shadcn/ui)
â”œâ”€â”€ scrapers/             # Scripts de scraping
â”œâ”€â”€ Dockerfile            # Configuration Docker
â”œâ”€â”€ next.config.mjs       # Configuration Next.js
â””â”€â”€ package.json          # DÃ©pendances
```

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©ez une branche feature (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## âš ï¸ Avertissement lÃ©gal

Ce projet est destinÃ© Ã  un usage Ã©ducatif et personnel uniquement. Assurez-vous de respecter les conditions d'utilisation des sites web que vous scrapez et les lois locales sur la collecte de donnÃ©es.

## ğŸ†˜ Support

Pour toute question ou problÃ¨me, ouvrez une issue sur GitHub. 